

import os
import subprocess
from pathlib import Path


SCRIPT_DIR = Path(__file__).resolve().parent



# 1. æ¨¡å‹è·¯å¾„é…ç½®
TEACHER_MODEL_PATH = ""  
STUDENT_MODEL_PATH = "" 

# 2. æ•°æ®é›†é…ç½®
DATASET_NAME = "train" 

DATASET_FILES_PER_ITERATION = [
    "data/medqa/train_id.jsonl", 
    "data/medmcqa/converted_train_id.jsonl", 
     "data/CMExam/CMExam_train_id.jsonl", 
]


ABILITY_CLASS_FILES_PER_ITERATION = [
    "ablity_tree/data/medqa/medqa_train_tree_sorted.jsonl", 
    "ablity_tree/data/medmcqa/medmcqa_train_tree.jsonl", 
    
]

NUM_ITERATIONS = 2  
GPUS_TO_USE = [0, 1, 2] 
NUM_DISTILL_SAMPLES = 10000 

# 4. åˆå§‹èƒ½åŠ›æ ‘é…ç½®
EXISTING_ABILITY_TREE_PATH = ""

# 5. è®­ç»ƒè¶…å‚æ•° (å¯åœ¨æ­¤å¤„ç»Ÿä¸€ç®¡ç†)

TRAIN_EPOCHS = 3
LEARNING_RATE = 5e-6
BATCH_SIZE = 2 
GRADIENT_ACCUMULATION_STEPS = 16 

# 6. æœ€ç»ˆäº§å‡º
WORKFLOW_OUTPUT_DIR = f""
MODELS_OUTPUT_DIR = f""


# --- é…ç½®ç»“æŸ ---
# ==============================================================================


class WorkflowOrchestrator:
    """
    ç®¡ç†å’Œæ‰§è¡Œæ•´ä¸ªçŸ¥è¯†è’¸é¦å·¥ä½œæµçš„ç±»ã€‚
    """
    def __init__(self, config):
        self.config = config
        self.current_student_path = Path(config["student_model_path"])
        self.iteration_states = {} # å­˜å‚¨æ¯ä¸€è½®çš„äº§å‡ºæ–‡ä»¶è·¯å¾„
        self.script_dir = SCRIPT_DIR
        
        # <--- MODIFIED: åˆ›å»ºä¸¤ä¸ªç‹¬ç«‹çš„è¾“å‡ºç›®å½•
        self.workdir = Path(config["workflow_output_dir"])
        self.models_dir = Path(config["models_output_dir"])
        self.workdir.mkdir(parents=True, exist_ok=True)
        self.models_dir.mkdir(parents=True, exist_ok=True)
        print(f"å·¥ä½œç›®å½• (æ—¥å¿—/ä¸­é—´æ•°æ®) å·²åˆ›å»º: {self.workdir.resolve()}")
        print(f"æ¨¡å‹ç›®å½• (Checkpoints/æœ€ç»ˆæ¨¡å‹) å·²åˆ›å»º: {self.models_dir.resolve()}")

    def _run_command(self, step_name: str, command: list, output_paths: list = None):
        """
        é€šç”¨å‘½ä»¤æ‰§è¡Œå‡½æ•°, æ–°å¢æ£€æŸ¥ç‚¹åŠŸèƒ½ã€‚
        output_paths: ä¸€ä¸ªåŒ…å«æ­¤æ­¥éª¤é¢„æœŸäº§å‡ºæ–‡ä»¶/ç›®å½•çš„åˆ—è¡¨ã€‚
        """
        # --- æ–°å¢ï¼šæ£€æŸ¥ç‚¹åŠŸèƒ½ ---
        if output_paths:
            # Path.exists() å¯¹æ–‡ä»¶å’Œç›®å½•éƒ½æœ‰æ•ˆ
            all_outputs_exist = all(Path(p).exists() for p in output_paths)
            if all_outputs_exist:
                print("\n" + "="*80)
                print(f"âœ… [æ­¥éª¤: {step_name}] æ£€æµ‹åˆ°æ‰€æœ‰è¾“å‡ºæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡æ­¤æ­¥éª¤ã€‚")
                print(f"   - å·²æ£€æŸ¥çš„è·¯å¾„: {', '.join(map(str, output_paths))}")
                print("="*80)
                return  # è·³è¿‡æ‰§è¡Œ

        print("\n" + "="*80)
        print(f"ğŸš€ [æ­¥éª¤: {step_name}] å¼€å§‹æ‰§è¡Œ...")
        print(f"   - å‘½ä»¤: {' '.join(map(str, command))}")
        
        log_filename = step_name.replace(':', '').replace(' ', '_').replace('-', '_')
        log_path = self.workdir / f"{log_filename}.log"
        print(f"   - æ—¥å¿—æ–‡ä»¶: {log_path}")


        my_env = os.environ.copy()
        if "accelerate" in command:
            my_env["CUDA_VISIBLE_DEVICES"] = ",".join(map(str, self.config['gpus']))

        try:
            # æ³¨æ„: æ­¤å¤„æ·»åŠ äº† env=my_env
            with open(log_path, 'w', encoding='utf-8') as log_file:
                process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, encoding='utf-8', bufsize=1, env=my_env)
                for line in iter(process.stdout.readline, ''):
                    print(f"   | {line.strip()}")
                    log_file.write(line)
                process.wait()
                rc = process.poll()

                if rc == 0:
                    print(f"âœ… [æ­¥éª¤: {step_name}] æ‰§è¡ŒæˆåŠŸï¼")
                else:
                    print(f"âŒ [æ­¥éª¤: {step_name}] æ‰§è¡Œå¤±è´¥ï¼è¿”å›ç : {rc}ã€‚è¯¦æƒ…è¯·æŸ¥çœ‹æ—¥å¿—: {log_path}")
                    raise RuntimeError(f"Step '{step_name}' failed.")
        except Exception as e:
            print(f"ğŸ’¥ [æ­¥éª¤: {step_name}] å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
            raise
    
    def run_step_1_create_initial_tree(self):
        """æ­¥éª¤1: é“¾æ¥å·²æœ‰çš„åˆå§‹èƒ½åŠ›æ ‘"""
        source_tree_path_str = self.config.get("existing_ability_tree_path")
        if not source_tree_path_str:
            raise ValueError("é”™è¯¯: è¯·åœ¨å…¨å±€é…ç½® 'EXISTING_ABILITY_TREE_PATH' ä¸­æŒ‡å®šæ‚¨ç°æœ‰èƒ½åŠ›æ ‘çš„æ­£ç¡®è·¯å¾„ã€‚")

        source_tree_path = Path(source_tree_path_str)
        target_tree_path = self.workdir / "initial_ability_tree.json"
        self.iteration_states['initial_tree'] = str(target_tree_path)

        print("\n" + "="*80)
        print("ğŸš€ [æ­¥éª¤: é“¾æ¥åˆå§‹èƒ½åŠ›æ ‘] å¼€å§‹æ‰§è¡Œ...")
        print(f"   - æºæ–‡ä»¶: {source_tree_path}")
        print(f"   - ç›®æ ‡ä½ç½®: {target_tree_path}")

        if target_tree_path.exists() or target_tree_path.is_symlink():
            print(f"âœ… [æ­¥éª¤: é“¾æ¥åˆå§‹èƒ½åŠ›æ ‘] ç›®æ ‡æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡é“¾æ¥ã€‚")
            return

        if not source_tree_path.is_file():
            source_tree_path = self.script_dir / source_tree_path
            if not source_tree_path.is_file():
                 raise FileNotFoundError(f"é”™è¯¯: æŒ‡å®šçš„åˆå§‹èƒ½åŠ›æ ‘æ–‡ä»¶ä¸å­˜åœ¨: {self.config.get('existing_ability_tree_path')}")
        try:
            os.link(source_tree_path, target_tree_path)
            print(f"âœ… [æ­¥éª¤: é“¾æ¥åˆå§‹èƒ½åŠ›æ ‘] æˆåŠŸé“¾æ¥æ–‡ä»¶ï¼")
        except Exception as e:
            print(f"âŒ [æ­¥éª¤: é“¾æ¥åˆå§‹èƒ½åŠ›æ ‘] é“¾æ¥æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            raise

    def run_iteration(self, iter_num: int):
        """æ‰§è¡Œå•æ¬¡å®Œæ•´çš„è¿­ä»£"""
        print("\n" + "#"*100)
        print(f"###   å¼€å§‹ç¬¬ {iter_num} è½®è¿­ä»£   ###")
        print("#"*100)
        
        # ç¡®å®šæœ¬è½®è¿­ä»£ä½¿ç”¨çš„å­¦ç”Ÿæ¨¡å‹è·¯å¾„
        # å¦‚æœä¸æ˜¯ç¬¬ä¸€è½®ï¼Œåˆ™å­¦ç”Ÿæ¨¡å‹æ˜¯ä¸Šä¸€è½®çš„äº§å‡º
        if iter_num > 1:
            self.current_student_path = self.models_dir / f"iteration_{iter_num - 1}_student_model" / "final_model"
        
        print(f"æœ¬è½®è¿­ä»£ä½¿ç”¨çš„å­¦ç”Ÿæ¨¡å‹: {self.current_student_path}")
        if not self.current_student_path.exists():
             # å¯¹äºç¬¬ä¸€è½®ï¼Œåˆå§‹å­¦ç”Ÿæ¨¡å‹å¿…é¡»å­˜åœ¨
            if iter_num == 1:
                raise FileNotFoundError(f"é”™è¯¯: åˆå§‹å­¦ç”Ÿæ¨¡å‹åœ¨è·¯å¾„ '{self.current_student_path}' æœªæ‰¾åˆ°ï¼")
            else:
                raise FileNotFoundError(f"é”™è¯¯: æ‰¾ä¸åˆ°ä¸Šä¸€è½® ({iter_num - 1}) è®­ç»ƒäº§å‡ºçš„å­¦ç”Ÿæ¨¡å‹ '{self.current_student_path}'ã€‚è¯·æ£€æŸ¥ä¸Šä¸€è½®æ˜¯å¦æˆåŠŸå®Œæˆã€‚")


        current_dataset_file = self.config['dataset_files'][iter_num - 1]
        current_ability_class_file = self.config['ability_class_files'][iter_num - 1]
        print(f"æœ¬è½®è¿­ä»£ä½¿ç”¨çš„æ•°æ®é›†: {current_dataset_file}")
        
        iter_workdir = self.workdir / f"iteration_{iter_num}"
        iter_workdir.mkdir(exist_ok=True)
        
        gpus_str = ",".join(map(str, self.config['gpus']))
        student_model_name = Path(self.current_student_path).name

        # --- å­æ­¥éª¤ A: è¯„ä¼°å½“å‰å­¦ç”Ÿæ¨¡å‹ (è°ƒç”¨ inference.py) ---
        student_reply_dir = iter_workdir / "A_student_reply"
        student_reply_dir.mkdir(exist_ok=True)
        student_replies_file = student_reply_dir / f"{student_model_name}_iter{iter_num}_{self.config['dataset_name']}_replies.jsonl"
        
        cmd_step_a = [
            "python", str(self.script_dir / "inference.py"),
            "--model_path", str(self.current_student_path),
            "--dataset_path", current_dataset_file,
            "--output_path", str(student_replies_file),
            "--gpu_ids", gpus_str
        ]
        self._run_command(f"Iter {iter_num}: Step A - Evaluate Student", cmd_step_a, output_paths=[student_replies_file])

        # --- å­æ­¥éª¤ B: åˆ†æå›ç­”ï¼Œç”Ÿæˆå­¦ç”Ÿèƒ½åŠ›æ ‘ (è°ƒç”¨ student_data_analyzer.py) ---
        student_tree_dir = iter_workdir / "B_student_tree"
        student_tree_dir.mkdir(exist_ok=True)
        student_ability_tree_file = student_tree_dir / f"{student_model_name}_iter{iter_num}_{self.config['dataset_name']}_tree.json"
        student_stats_file = student_tree_dir / f"{student_model_name}_iter{iter_num}_{self.config['dataset_name']}_stats.json"
        
        cmd_step_b = [
            "python", str(self.script_dir / "student_data_analyzer.py"),
            "--initial_tree_path", self.iteration_states['initial_tree'],
            "--paths_file", current_ability_class_file,
            "--answers_file", current_dataset_file,
            "--student_responses_file", str(student_replies_file),
            "--output_tree_path", str(student_ability_tree_file),
            "--output_stats_path", str(student_stats_file)
        ]
        self._run_command(f"Iter {iter_num}: Step B - Analyze Ability", cmd_step_b, output_paths=[student_ability_tree_file, student_stats_file])

        # --- å­æ­¥éª¤ C.1: ç”Ÿæˆè’¸é¦æ•°æ® (è°ƒç”¨ generate_distill_data.py) ---
        distill_data_dir = iter_workdir / "C_distill_data"
        distill_data_dir.mkdir(exist_ok=True)
        generated_data_file = distill_data_dir / "generated_data.jsonl"
        
        cmd_step_c1 = [
            "python", str(self.script_dir / "generate_distill_data.py"),
            "--teacher_model_path", self.config['teacher_model_path'],
            "--ability_tree_path", str(student_ability_tree_file),
            "--full_dataset_path", current_dataset_file,
            "--ability_class_path", current_ability_class_file,
            "--output_path", str(generated_data_file),
            "--num_samples", str(self.config['num_distill_samples']),
            "--gpu_ids", gpus_str
        ]
        self._run_command(f"Iter {iter_num}: Step C.1 - Generate Data", cmd_step_c1, output_paths=[generated_data_file])

        # --- å­æ­¥éª¤ C.2: ä¸ºè’¸é¦æ•°æ®ç”Ÿæˆæ•™å¸ˆ Logits (è°ƒç”¨ generate_logits.py) ---
        logits_main_dir = distill_data_dir / "teacher_logits_and_report"
        teacher_logits_dir = logits_main_dir / "logits_pt"
        
        cmd_step_c2 = [
            "python", str(self.script_dir / "generate_logits.py"),
            "--model_path", self.config['teacher_model_path'],
            "--dataset_path", str(generated_data_file),
            "--output_dir", str(logits_main_dir),
            "--gpu_ids", gpus_str
        ]
        self._run_command(f"Iter {iter_num}: Step C.2 - Generate Logits", cmd_step_c2, output_paths=[teacher_logits_dir])

        # --- å­æ­¥éª¤ D: éªŒè¯è’¸é¦æ•°æ® (è°ƒç”¨ evaluate_distilled_data.py) ---
        validated_data_dir = iter_workdir / "D_validated_data"
        validated_data_dir.mkdir(exist_ok=True)
        validated_data_file = validated_data_dir / "validated_data.jsonl"
        
        cmd_step_d = [
            "python", str(self.script_dir / "evaluate_distilled_data.py"),
            "--agent_model_1_path", self.config['teacher_model_path'],
            "--agent_model_2_path", self.config['teacher_model_path'],
            "--input_file", str(generated_data_file),
            "--output_file", str(validated_data_file),
            "--gpu_ids", gpus_str
        ]
        self._run_command(f"Iter {iter_num}: Step D - Validate Data", cmd_step_d, output_paths=[validated_data_file])

        # --- å­æ­¥éª¤ E: è®­ç»ƒå­¦ç”Ÿæ¨¡å‹ (è°ƒç”¨ train.py) ---
        new_student_model_dir = self.models_dir / f"iteration_{iter_num}_student_model"
        final_model_output_path = new_student_model_dir / "final_model"
        
        num_gpus = len(self.config['gpus'])
        cmd_step_e = [
            "accelerate", "launch",
            f"--num_processes={num_gpus}", # æŒ‡å®šè¿›ç¨‹æ•°=GPUæ•°
            f"--main_process_port={29500 + iter_num}", # ä¸ºæ¯æ¬¡è¿­ä»£åˆ†é…ä¸åŒç«¯å£ï¼Œé¿å…å†²çª
            str(self.script_dir / "train.py"),
            "--student_model_path", str(self.current_student_path),
            "--train_file_path", str(validated_data_file),
            "--teacher_logits_dir", str(teacher_logits_dir),
            "--output_dir", str(new_student_model_dir),
            "--num_epochs", str(self.config['train_epochs']),
            "--learning_rate", str(self.config['learning_rate']),
            "--batch_size", str(self.config['batch_size']),
            "--gradient_accumulation_steps", str(self.config['gradient_accumulation_steps']),
        ]
        self._run_command(f"Iter {iter_num}: Step E - Train Student", cmd_step_e, output_paths=[final_model_output_path])
        
        # æ›´æ–°å½“å‰å­¦ç”Ÿæ¨¡å‹è·¯å¾„ä»¥å¤‡ä¸‹ä¸€è½®è¿­ä»£
        self.current_student_path = final_model_output_path
        
        print(f"ğŸ‰ ç¬¬ {iter_num} è½®è¿­ä»£å®Œæˆï¼æ–°çš„å­¦ç”Ÿæ¨¡å‹ä½äº: {self.current_student_path}")

    def run(self):
        """å¯åŠ¨æ•´ä¸ªå·¥ä½œæµ"""
        print("====== å¯åŠ¨ AT-KD è‡ªåŠ¨åŒ–å·¥ä½œæµ ======")
        
        # 1. é“¾æ¥æˆ–éªŒè¯åˆå§‹èƒ½åŠ›æ ‘
        self.run_step_1_create_initial_tree()
        
        # 2. å¼€å§‹è¿­ä»£
        for i in range(1, self.config["num_iterations"] + 1):
            self.run_iteration(i)

        print("\n\n" + "#"*100)
        print("###   AT-KD è‡ªåŠ¨åŒ–å·¥ä½œæµå…¨éƒ¨æ‰§è¡Œå®Œæ¯•   ###")
        print(f"###   æœ€ç»ˆæ—¥å¿—å’Œæ•°æ®ä¿å­˜åœ¨: {self.workdir.resolve()}   ###")
        print(f"###   æœ€ç»ˆå­¦ç”Ÿæ¨¡å‹ä¿å­˜åœ¨: {self.models_dir.resolve()}   ###")
        print("#"*100)


if __name__ == "__main__":
    # æ£€æŸ¥é…ç½®æ˜¯å¦åŒ¹é…
    if len(DATASET_FILES_PER_ITERATION) != NUM_ITERATIONS or len(ABILITY_CLASS_FILES_PER_ITERATION) != NUM_ITERATIONS:
        raise ValueError(
            f"é…ç½®é”™è¯¯: 'DATASET_FILES_PER_ITERATION' å’Œ 'ABILITY_CLASS_FILES_PER_ITERATION' åˆ—è¡¨çš„é•¿åº¦ "
            f"({len(DATASET_FILES_PER_ITERATION)}, {len(ABILITY_CLASS_FILES_PER_ITERATION)}) "
            f"å¿…é¡»ä¸ 'NUM_ITERATIONS' ({NUM_ITERATIONS}) ç›¸ç­‰ã€‚"
        )

    # å°†é…ç½®å­—å…¸åŒ–
    main_config = {
        "teacher_model_path": TEACHER_MODEL_PATH,
        "student_model_path": STUDENT_MODEL_PATH,
        "dataset_name": DATASET_NAME,
        "dataset_files": DATASET_FILES_PER_ITERATION,
        "ability_class_files": ABILITY_CLASS_FILES_PER_ITERATION,
        "num_iterations": NUM_ITERATIONS,
        "gpus": GPUS_TO_USE,
        "workflow_output_dir": WORKFLOW_OUTPUT_DIR,
        "models_output_dir": MODELS_OUTPUT_DIR,
        "existing_ability_tree_path": EXISTING_ABILITY_TREE_PATH,
        "num_distill_samples": NUM_DISTILL_SAMPLES,
        "train_epochs": TRAIN_EPOCHS,
        "learning_rate": LEARNING_RATE,
        "batch_size": BATCH_SIZE,
        "gradient_accumulation_steps": GRADIENT_ACCUMULATION_STEPS,
    }
    
    orchestrator = WorkflowOrchestrator(main_config)
    orchestrator.run()