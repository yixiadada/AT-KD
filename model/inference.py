

import json
import os
from pathlib import Path
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from tqdm import tqdm
import multiprocessing
import numpy as np
import time
import argparse 


def build_qwen2_prompt(tokenizer, question: str, options: dict):
    """æ„å»ºä¸è®­ç»ƒè„šæœ¬ä¸€è‡´çš„Promptã€‚"""
    options_str = "\n".join([f"{key}. {value}" for key, value in sorted(options.items())])
    prompt_text = (
        f"ä½ æ˜¯ä¸€ä½é¡¶çº§çš„åŒ»å­¦ä¸“å®¶ã€‚è¯·ä»”ç»†åˆ†æä»¥ä¸‹åŒ»å­¦å¤šé¡¹é€‰æ‹©é¢˜ï¼Œå¹¶ç›´æ¥é€‰æ‹©æœ€æ­£ç¡®çš„é€‰é¡¹å­—æ¯ã€‚\n\n"
        f"é—®é¢˜: {question}\n\n"
        f"é€‰é¡¹:\n{options_str}\n\n"
        f"æ­£ç¡®ç­”æ¡ˆçš„é€‰é¡¹æ˜¯ï¼š"
    )
    messages = [{"role": "user", "content": prompt_text}]
    full_prompt = tokenizer.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=True
    )
    return full_prompt

def extract_option_letter(reply: str, options: dict):
    """ä»æ¨¡å‹å›å¤ä¸­æå–é€‰é¡¹å­—æ¯ (A, B, C, D, E...)"""
    r = reply.strip().upper()
    for k in options:
        if r.startswith(k):
            return k
    for k in options:
        if k in r:
            return k
    return ""

def load_model_for_worker(model_dir: Path, device: str):
    """ä¸ºå•ä¸ªå·¥ä½œè¿›ç¨‹åŠ è½½æ¨¡å‹åˆ°æŒ‡å®šçš„è®¾å¤‡ã€‚"""
    print(f"[INFO] Process-{os.getpid()} is loading model to {device}...")
    tokenizer = AutoTokenizer.from_pretrained(str(model_dir), trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        str(model_dir),
        torch_dtype=torch.bfloat16,
        trust_remote_code=True
    ).to(device)
    
    model.eval()
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    return tokenizer, model

# --- æ ¸å¿ƒå·¥ä½œè¿›ç¨‹å‡½æ•° ---
def inference_worker(gpu_id: int, data_chunk: list, model_path: Path, all_gpu_ids: list, max_new_tokens: int):
    """
    åœ¨å•ä¸ªGPUä¸Šè¿è¡Œæ¨ç†çš„å·¥ä½œå‡½æ•°ã€‚
    """
    device = f"cuda:{gpu_id}"
    
    # å°†IDæœ€å°çš„GPUè¿›ç¨‹ä½œä¸ºè°ƒè¯•è¾“å‡ºè¿›ç¨‹
    is_debug_process = (gpu_id == min(all_gpu_ids))
    
    try:
        tokenizer, model = load_model_for_worker(model_path, device)
        
        outputs, answers, records = [], [], []
        
        progress_bar = tqdm(
            data_chunk, 
            desc=f"GPU-{gpu_id} æ¨ç†ä¸­", 
            position=gpu_id, 
            unit="é¢˜"
        )
        
        for local_idx, item in enumerate(progress_bar):
            prompt = build_qwen2_prompt(tokenizer, item["question"], item["options"])
            inputs = tokenizer(prompt, return_tensors="pt").to(device)

            with torch.no_grad():
                generated_ids = model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    do_sample=False,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            out_text = tokenizer.decode(
                generated_ids[0, inputs["input_ids"].shape[1]:],
                skip_special_tokens=True
            )

            # æ–°å¢ï¼šç¡®ä¿ item ä¸­æœ‰ 'id' å­—æ®µï¼Œç”¨äºåç»­åŒ¹é…
            item_id = item.get('id', f'item_{local_idx}')
            answer_letter = item["answer_idx"].strip().upper()
            pred_letter = extract_option_letter(out_text, item["options"])

            if is_debug_process and local_idx % 20 == 0: # æ¯20ä¸ªé—®é¢˜æ‰“å°ä¸€æ¬¡è°ƒè¯•ä¿¡æ¯
                print(f"\n----- [GPU {gpu_id}] é¢˜ç›® ID:{item_id} -----")
                print(f"æ¨¡å‹åŸå§‹å›å¤: '{out_text.strip()}'")
                print(f"æå–å‡ºçš„é€‰é¡¹: '{pred_letter}'")
                print(f"æ­£ç¡®ç­”æ¡ˆé€‰é¡¹: '{answer_letter}'")
                print(f"ç»“æœ: {'æ­£ç¡®' if pred_letter == answer_letter else 'é”™è¯¯'}")
                print("-" * (30 + len(str(item_id))))

            record = {
                "id": item_id, # ç¡®ä¿IDè¢«è®°å½•
                "question": item["question"],
                "options": item["options"],
                "answer_letter": answer_letter,
                "answer_text": item["options"].get(answer_letter, "N/A"),
                "model_reply": out_text.strip(),
                "pred_letter": pred_letter
            }
            
            outputs.append(pred_letter)
            answers.append(answer_letter)
            records.append(record)
            
        return outputs, answers, records
    except Exception as e:
        print(f"[ERROR] GPU-{gpu_id} process failed: {e}")
        import traceback
        traceback.print_exc()
        return [], [], []

# --- ä»»åŠ¡ç¼–æ’å‡½æ•° ---
def run_evaluation(model_path: Path, test_file_path: Path, output_path: Path, gpu_ids: list, max_new_tokens: int):
    """å¯¹å•ä¸ªæ•°æ®é›†æ‰§è¡Œå®Œæ•´çš„è¯„æµ‹æµç¨‹ã€‚"""
    print("\n" + "="*80)
    print(f"ğŸš€ å¼€å§‹æ¨¡å‹è¯„ä¼°ä»»åŠ¡")
    print(f"   - æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"   - æ•°æ®æ–‡ä»¶: {test_file_path}")
    print(f"   - è¾“å‡ºæ–‡ä»¶: {output_path}")
    print(f"   - ä½¿ç”¨ GPUs: {gpu_ids}")
    print("="*80 + "\n")

    start_time = time.time()
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_path.parent.mkdir(parents=True, exist_ok=True)
    preds_path = output_path.with_suffix('.preds.txt') # é¢„æµ‹é€‰é¡¹çš„é™„å±æ–‡ä»¶

    # 1. åŠ è½½å¹¶åˆ‡åˆ†æ•°æ®
    print(f"[INFO] æ­£åœ¨åŠ è½½å’Œåˆ‡åˆ†æ•°æ®...")
    try:
        with test_file_path.open(encoding="utf8") as f:
            test_data = [json.loads(line) for line in f]
    except FileNotFoundError:
        print(f"[ERROR] æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°: {test_file_path}")
        return

    num_gpus = len(gpu_ids)
    data_chunks = np.array_split(test_data, num_gpus)
    data_chunks = [chunk.tolist() for chunk in data_chunks]

    print(f"[INFO] æ•°æ®å·²åˆ‡åˆ†ä¸º {num_gpus} å—ï¼Œå¼€å§‹å¹¶è¡Œæ¨ç†...")

    # 2. åˆ›å»ºå¹¶è¿è¡Œå¤šè¿›ç¨‹
    tasks = [(gpu_ids[i], data_chunks[i], model_path, gpu_ids, max_new_tokens) for i in range(num_gpus)]
    
    multiprocessing.set_start_method("spawn", force=True)
    with multiprocessing.Pool(processes=num_gpus) as pool:
        results = pool.starmap(inference_worker, tasks)

    # 3. æ±‡æ€»ç»“æœ
    print(f"\n[INFO] æ­£åœ¨æ±‡æ€»æ‰€æœ‰è¿›ç¨‹çš„ç»“æœ...")
    all_outputs, all_answers, all_records = [], [], []
    for outputs, answers, records in results:
        all_outputs.extend(outputs)
        all_answers.extend(answers)
        all_records.extend(records)

    # 4. å†™å…¥æ–‡ä»¶å’Œç»Ÿè®¡
    with output_path.open("w", encoding="utf8") as f_jsonl:
        for record in all_records:
            json.dump(record, f_jsonl, ensure_ascii=False)
            f_jsonl.write("\n")
            
    with preds_path.open("w", encoding="utf8") as f_preds:
        for pred in all_outputs:
            f_preds.write(pred + "\n")

    total_q = len(all_outputs)
    correct_q = sum(p == a for p, a in zip(all_outputs, all_answers))
    acc = correct_q / total_q if total_q else 0.0

    stats = f"\næ€»é¢˜æ•°: {total_q}\nç­”å¯¹é¢˜æ•°: {correct_q}\nå‡†ç¡®ç‡: {acc:.2%}"
    
    end_time = time.time()
    elapsed_time = end_time - start_time
    
    print("\n" + "="*80)
    print(f"è¯„ä¼°å®Œæˆï¼")
    print(stats)
    print(f"æ€»è€—æ—¶: {elapsed_time:.2f} ç§’")
    
    with output_path.open("a", encoding="utf8") as f:
        f.write(stats)
        f.write(f"\næ€»è€—æ—¶: {elapsed_time:.2f} ç§’")

    print(f"\nè¯„ä¼°ç»“æœå·²ä¿å­˜:")
    print(f"  - è¯¦ç»†å›å¤æ–‡ä»¶: {output_path}")
    print(f"  - é¢„æµ‹é€‰é¡¹æ–‡ä»¶: {preds_path}")
    print("="*80)


def main():
    """ä¸»å‡½æ•°ï¼Œè§£æå‘½ä»¤è¡Œå‚æ•°å¹¶å¯åŠ¨è¯„ä¼°æµç¨‹ã€‚"""
    parser = argparse.ArgumentParser(description="åœ¨å¤šä¸ªGPUä¸Šå¹¶è¡Œè¿è¡Œè¯­è¨€æ¨¡å‹è¯„ä¼°ã€‚")
    parser.add_argument("--model_path", type=str, required=True, help="è¦è¯„ä¼°çš„æ¨¡å‹çš„è·¯å¾„ã€‚")
    parser.add_argument("--dataset_path", type=str, required=True, help="è¯„ä¼°æ•°æ®é›†ï¼ˆ.jsonlæ ¼å¼ï¼‰çš„è·¯å¾„ã€‚")
    parser.add_argument("--output_path", type=str, required=True, help="ä¿å­˜è¯¦ç»†è¯„ä¼°ç»“æœï¼ˆ.jsonlæ ¼å¼ï¼‰çš„è¾“å‡ºæ–‡ä»¶è·¯å¾„ã€‚")
    parser.add_argument("--gpu_ids", type=str, required=True, help="ç”¨äºæ¨ç†çš„GPU IDåˆ—è¡¨ï¼Œä»¥é€—å·åˆ†éš”ï¼ˆä¾‹å¦‚ '0,1,2'ï¼‰ã€‚")
    parser.add_argument("--max_new_tokens", type=int, default=1024, help="æ¨¡å‹ç”Ÿæˆæ–°tokençš„æœ€å¤§æ•°é‡ã€‚")
    
    args = parser.parse_args()

    # è§£æGPU IDåˆ—è¡¨
    try:
        gpu_ids = [int(gid.strip()) for gid in args.gpu_ids.split(',')]
        if not gpu_ids:
            raise ValueError
    except (ValueError, IndexError):
        print("é”™è¯¯: 'gpu_ids' å‚æ•°æ ¼å¼ä¸æ­£ç¡®ã€‚è¯·æä¾›ä¸€ä¸ªé€—å·åˆ†éš”çš„æ•°å­—åˆ—è¡¨ï¼Œä¾‹å¦‚ '0,1,2'ã€‚")
        return

    # å¯åŠ¨è¯„ä¼°
    run_evaluation(
        model_path=Path(args.model_path),
        test_file_path=Path(args.dataset_path),
        output_path=Path(args.output_path),
        gpu_ids=gpu_ids,
        max_new_tokens=args.max_new_tokens
    )

if __name__ == "__main__":
    main()