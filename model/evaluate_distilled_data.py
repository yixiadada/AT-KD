

import json
import os
from pathlib import Path
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from tqdm import tqdm
import multiprocessing
import numpy as np
import time
import argparse
import tempfile

def build_prompt(tokenizer, question: str, options: dict):
    """æ„å»ºæ ‡å‡†çš„è¯„ä¼°Promptã€‚"""
    options_str = "\n".join([f"{key}. {value}" for key, value in sorted(options.items())])
    prompt_text = (
        f"ä½ æ˜¯ä¸€ä½é¡¶çº§çš„åŒ»å­¦ä¸“å®¶ã€‚è¯·ä»”ç»†åˆ†æä»¥ä¸‹åŒ»å­¦å¤šé¡¹é€‰æ‹©é¢˜ï¼Œå¹¶ç›´æ¥é€‰æ‹©æœ€æ­£ç¡®çš„é€‰é¡¹å­—æ¯ã€‚\n\n"
        f"é—®é¢˜: {question}\n\n"
        f"é€‰é¡¹:\n{options_str}\n\n"
        f"æ­£ç¡®ç­”æ¡ˆçš„é€‰é¡¹æ˜¯ï¼š"
    )
    messages = [{"role": "user", "content": prompt_text}]
    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

def extract_option_letter(reply: str, options: dict):
    """ä»æ¨¡å‹å›å¤ä¸­ç¨³å¥åœ°æå–é€‰é¡¹å­—æ¯ã€‚"""
    r = reply.strip().upper()
    for k in options:
        if r.startswith(k): return k
    for k in options:
        if k in r: return k
    return ""

def load_model_for_worker(model_dir: Path, device: str):
    """ä¸ºå•ä¸ªå·¥ä½œè¿›ç¨‹åŠ è½½æ¨¡å‹åˆ°æŒ‡å®šè®¾å¤‡ã€‚"""
    print(f"[INFO] Process-{os.getpid()} is loading model to {device}...")
    tokenizer = AutoTokenizer.from_pretrained(str(model_dir), trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        str(model_dir), torch_dtype=torch.bfloat16, trust_remote_code=True
    ).to(device)
    model.eval()
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    return tokenizer, model

def inference_worker(gpu_id: int, data_chunk: list, model_path: Path, max_new_tokens: int, worker_queue):
    """åœ¨å•ä¸ªGPUä¸Šè¿è¡Œæ¨ç†çš„å·¥ä½œå‡½æ•°ï¼Œå¹¶å°†ç»“æœæ”¾å…¥é˜Ÿåˆ—ã€‚"""
    device = f"cuda:{gpu_id}"
    try:
        tokenizer, model = load_model_for_worker(model_path, device)
        
        for item in tqdm(data_chunk, desc=f"GPU-{gpu_id}", position=gpu_id, unit="æ¡"):
            prompt = build_prompt(tokenizer, item["question"], item["options"])
            inputs = tokenizer(prompt, return_tensors="pt").to(device)

            with torch.no_grad():
                generated_ids = model.generate(
                    **inputs, max_new_tokens=max_new_tokens, do_sample=False, pad_token_id=tokenizer.eos_token_id
                )
            
            out_text = tokenizer.decode(generated_ids[0, inputs["input_ids"].shape[1]:], skip_special_tokens=True)
            pred_letter = extract_option_letter(out_text, item["options"])
            
            # å°†ç»“æœæ”¾å…¥é˜Ÿåˆ—
            worker_queue.put({"id": item.get('id'), "pred_letter": pred_letter})
            
    except Exception as e:
        print(f"[ERROR] GPU-{gpu_id} process failed: {e}")

def run_single_agent_evaluation(model_path: Path, data_file: Path, gpu_ids: list, max_new_tokens: int) -> dict:
    """ä½¿ç”¨å•ä¸ªä»£ç†æ¨¡å‹å¯¹æ•°æ®è¿›è¡Œè¯„ä¼°ï¼Œå¹¶è¿”å›ä¸€ä¸ªä»¥IDä¸ºé”®çš„é¢„æµ‹ç»“æœå­—å…¸ã€‚"""
    print("\n" + "-"*80)
    print(f"ğŸ•µï¸  å¼€å§‹ä½¿ç”¨ä»£ç†è¿›è¡Œè¯„ä¼°: {model_path.name}")
    
    try:
        with data_file.open(encoding="utf8") as f:
            test_data = [json.loads(line) for line in f if line.strip()]
    except FileNotFoundError:
        print(f"[ERROR] æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°: {data_file}")
        return {}

    num_gpus = len(gpu_ids)
    data_chunks = np.array_split(test_data, num_gpus)
    data_chunks = [chunk.tolist() for chunk in data_chunks]

    manager = multiprocessing.Manager()
    results_queue = manager.Queue()
    
    tasks = [(gpu_ids[i], data_chunks[i], model_path, max_new_tokens, results_queue) for i in range(num_gpus)]
    
    multiprocessing.set_start_method("spawn", force=True)
    with multiprocessing.Pool(processes=num_gpus) as pool:
        pool.starmap(inference_worker, tasks)

    predictions = {}
    while not results_queue.empty():
        result = results_queue.get()
        if result['id'] is not None:
            predictions[str(result['id'])] = result
            
    print(f"âœ… ä»£ç†è¯„ä¼°å®Œæˆï¼Œå…±å¾—åˆ° {len(predictions)} æ¡é¢„æµ‹ç»“æœã€‚")
    print("-"*80)
    return predictions



def main():
    """ä¸»å‡½æ•°ï¼Œè´Ÿè´£ç¼–æ’åŒä»£ç†éªŒè¯æµç¨‹ã€‚"""
    parser = argparse.ArgumentParser(description="ä½¿ç”¨åŒä»£ç†äº¤å‰éªŒè¯è’¸é¦æ•°æ®çš„è´¨é‡ã€‚")
    parser.add_argument("--agent_model_1_path", type=str, required=True, help="ç¬¬ä¸€ä¸ªè¯„ä¼°ä»£ç†æ¨¡å‹çš„è·¯å¾„ã€‚")
    parser.add_argument("--agent_model_2_path", type=str, required=True, help="ç¬¬äºŒä¸ªè¯„ä¼°ä»£ç†æ¨¡å‹çš„è·¯å¾„ã€‚")
    parser.add_argument("--input_file", type=str, required=True, help="å¾…éªŒè¯çš„è’¸é¦æ•°æ®æ–‡ä»¶è·¯å¾„ (.jsonl)ã€‚")
    parser.add_argument("--output_file", type=str, required=True, help="ä¿å­˜é€šè¿‡éªŒè¯çš„æ•°æ®çš„è¾“å‡ºæ–‡ä»¶è·¯å¾„ (.jsonl)ã€‚")
    parser.add_argument("--gpu_ids", type=str, required=True, help="ç”¨äºæ¨ç†çš„GPU IDåˆ—è¡¨ï¼Œä»¥é€—å·åˆ†éš”ï¼ˆä¾‹å¦‚ '0,1,2'ï¼‰ã€‚")
    parser.add_argument("--max_new_tokens", type=int, default=256, help="æ¨¡å‹ç”Ÿæˆæ–°tokençš„æœ€å¤§æ•°é‡ã€‚")

    args = parser.parse_args()

    # --- è§£æå‚æ•° ---
    try:
        gpu_ids = [int(gid.strip()) for gid in args.gpu_ids.split(',')]
        if not gpu_ids: raise ValueError
    except (ValueError, IndexError):
        print("é”™è¯¯: 'gpu_ids' å‚æ•°æ ¼å¼ä¸æ­£ç¡®ã€‚è¯·æä¾›ä¸€ä¸ªé€—å·åˆ†éš”çš„æ•°å­—åˆ—è¡¨ï¼Œä¾‹å¦‚ '0,1,2'ã€‚")
        return

    agent_1_path = Path(args.agent_model_1_path)
    agent_2_path = Path(args.agent_model_2_path)
    input_file = Path(args.input_file)
    output_file = Path(args.output_file)
    output_file.parent.mkdir(parents=True, exist_ok=True)

    print("="*80)
    print("ğŸš€ å¼€å§‹è’¸é¦æ•°æ®åŒä»£ç†éªŒè¯æµç¨‹")
    print(f"   - è¾“å…¥æ–‡ä»¶: {input_file}")
    print(f"   - è¾“å‡ºæ–‡ä»¶: {output_file}")
    print("="*80)

    # --- æ­¥éª¤1: ä½¿ç”¨ä»£ç†1è¿›è¡Œè¯„ä¼° ---
    preds_agent_1 = run_single_agent_evaluation(agent_1_path, input_file, gpu_ids, args.max_new_tokens)

    # --- æ­¥éª¤2: ä½¿ç”¨ä»£ç†2è¿›è¡Œè¯„ä¼° ---
    preds_agent_2 = run_single_agent_evaluation(agent_2_path, input_file, gpu_ids, args.max_new_tokens)

    # --- æ­¥éª¤3: äº¤å‰éªŒè¯å¹¶å†™å…¥æœ€ç»ˆç»“æœ ---
    print("\nğŸ” å¼€å§‹äº¤å‰éªŒè¯...")
    
    validated_data = []
    total_count = 0
    with input_file.open('r', encoding='utf-8') as f:
        for line in f:
            if not line.strip(): continue
            original_item = json.loads(line)
            total_count += 1
            item_id = str(original_item.get('id'))
            
            # è·å–ä¸¤ä¸ªä»£ç†çš„é¢„æµ‹ç»“æœ
            pred_1 = preds_agent_1.get(item_id, {}).get('pred_letter')
            pred_2 = preds_agent_2.get(item_id, {}).get('pred_letter')
            
            # è·å–æ ‡å‡†ç­”æ¡ˆ
            ground_truth = original_item.get('answer_idx')
            
            # åªæœ‰å½“ä¸¤ä¸ªä»£ç†éƒ½é¢„æµ‹æ­£ç¡®æ—¶ï¼Œæ‰è®¤ä¸ºæ•°æ®æœ‰æ•ˆ
            if ground_truth and pred_1 == ground_truth and pred_2 == ground_truth:
                validated_data.append(original_item)

    # å†™å…¥é€šè¿‡éªŒè¯çš„æ•°æ®
    with output_file.open('w', encoding='utf-8') as f:
        for item in validated_data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
            
    pass_rate = (len(validated_data) / total_count) * 100 if total_count > 0 else 0
    
    print("\n" + "="*80)
    print("âœ… éªŒè¯æµç¨‹å…¨éƒ¨å®Œæˆï¼")
    print(f"   - æ€»å…±å¤„ç†æ•°æ®æ¡æ•°: {total_count}")
    print(f"   - é€šè¿‡åŒé‡éªŒè¯æ•°é‡: {len(validated_data)}")
    print(f"   - æ•°æ®é€šè¿‡ç‡: {pass_rate:.2f}%")
    print(f"   - é«˜è´¨é‡è’¸é¦æ•°æ®å·²ä¿å­˜è‡³: {output_file}")
    print("="*80)


if __name__ == "__main__":
    main()