

import json
import random
import torch
import numpy as np
from collections import defaultdict
from transformers import AutoTokenizer, AutoModelForCausalLM
import glob
import os
import torch.multiprocessing as mp
import re
from tqdm import tqdm
import argparse
import tempfile
from pathlib import Path


def load_local_model(model_path: str, gpu_id: str):
    tqdm.write(f"â³ [GPU {gpu_id}] æ­£åœ¨ä» '{model_path}' åŠ è½½æ¨¡å‹...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        model = AutoModelForCausalLM.from_pretrained(
            model_path, 
            torch_dtype=torch.bfloat16,
            device_map=f'cuda:{gpu_id}'
        )
        tqdm.write(f"âœ… [GPU {gpu_id}] æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        return model, tokenizer
    except Exception as e:
        tqdm.write(f"âŒ [GPU {gpu_id}] æ¨¡å‹åŠ è½½å¤±è´¥: {e}"); return None, None

def load_full_dataset(file_path: str):
    dataset_map = {}
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f: 
            if line.strip():
                data_item = json.loads(line)
                dataset_map[str(data_item['id'])] = data_item
    return dataset_map

def load_ability_classifications(file_path: str):
    ability_to_ids = defaultdict(list)
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                item = json.loads(line)
                ability_to_ids[tuple(item['ability_path'])].append(str(item['id']))
    return ability_to_ids

def load_ability_tree(file_path: str):
    with open(file_path, 'r', encoding='utf-8') as f: return json.load(f)

def get_leaf_abilities(node, path=None):
    if path is None: path = []
    leaf_scores = {}
    if not isinstance(node, dict):
        return {}
    for name, value in node.items():
        if name == 'ç»¼åˆå¾—åˆ†': continue
        if isinstance(value, dict): 
            leaf_scores.update(get_leaf_abilities(value, path + [name]))
        elif isinstance(value, (int, float)): 
            leaf_scores[tuple(path + [name])] = value
    return leaf_scores


def calculate_generation_allocation(ability_tree, total_count):
    leaf_scores = get_leaf_abilities(ability_tree)
    if not leaf_scores: return {}
    abilities, scores = list(leaf_scores.keys()), np.array(list(leaf_scores.values()))
    

    weights = 1.0 / (scores - np.min(scores) + 1e-6)
    normalized_weights = weights / np.sum(weights)
    
    raw_counts = normalized_weights * total_count
    counts = np.floor(raw_counts).astype(int)
    

    remainder = total_count - np.sum(counts)
    if remainder > 0:
        top_indices = np.argsort(raw_counts - counts)[-int(remainder):]
        counts[top_indices] += 1
        
    return {abilities[i]: counts[i] for i in range(len(abilities)) if counts[i] > 0}

def parse_generated_text_robustly(text_block: str, gpu_id: str):
    try:
        question_match = re.search(r"é¢˜å¹²[:ï¼š\s]*(.*?)\n", text_block, re.DOTALL)
        if not question_match: return None
        question_text = question_match.group(1).strip()

        options = {}
        option_matches = re.findall(r"([A-Z])\.\s*(.*?)\n", text_block)
        for match in option_matches:
            options[match[0]] = match[1].strip()
        if not options: return None

        answer_match = re.search(r"ç­”æ¡ˆ[:ï¼š\s]*([A-Z])", text_block)
        answer_idx = answer_match.group(1).strip() if answer_match else "N/A"
        answer_text = options.get(answer_idx, "N/A")

        return {
            "question": question_text, "options": options,
            "answer": answer_text, "answer_idx": answer_idx
        }
    except Exception as e:
        tqdm.write(f"âš ï¸ [GPU {gpu_id}] åœ¨å®½æ¾è§£ææ¨¡å¼ä¸‹ä»ç„¶å‡ºé”™: {e}")
        return None

def generate_similar_question(seed_question: dict, model, tokenizer, gpu_id):
    ability_path_str = " -> ".join(seed_question['ability_path'])
    options_str = "\n".join([f"{k}. {v}" for k, v in seed_question['options'].items()])
    prompt = f"""ä½ æ˜¯ä¸€ä½é¡¶çº§çš„åŒ»å­¦å‡ºé¢˜ä¸“å®¶ã€‚è¯·ä¸¥æ ¼æ¨¡ä»¿ä»¥ä¸‹â€œç§å­é—®é¢˜â€çš„é£æ ¼ã€çŸ¥è¯†ç‚¹ï¼ˆé¢†åŸŸï¼š{ability_path_str}ï¼‰å’Œæ ¼å¼ï¼Œåˆ›é€ 1é“å…¨æ–°çš„ã€é«˜è´¨é‡çš„å•é¡¹é€‰æ‹©é¢˜ã€‚\n\nè¦æ±‚ï¼š\n1. æ–°é—®é¢˜çš„è€ƒç‚¹ã€éš¾åº¦å’Œé€‰é¡¹æ•°é‡åº”ä¸ç§å­é—®é¢˜é«˜åº¦ç›¸ä¼¼ã€‚\n2. å¿…é¡»ä½¿ç”¨ä¸åŒçš„æé—®æ–¹å¼ã€æ¡ˆä¾‹æˆ–æƒ…æ™¯ï¼Œä¸¥ç¦æŠ„è¢­åŸæ–‡ã€‚\n3. ä¸¥æ ¼æŒ‰ç…§ä¸‹é¢çš„æ ¼å¼è¾“å‡ºï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–çš„è§£é‡Šæˆ–æ–‡å­—ã€‚\n\n[START]\né¢˜å¹²: [è¿™é‡Œæ˜¯æ–°é—®é¢˜çš„é¢˜å¹²]\nA. [é€‰é¡¹Açš„å†…å®¹]\nB. [é€‰é¡¹Bçš„å†…å®¹]\nC. [é€‰é¡¹Cçš„å†…å®¹]\nD. [é€‰é¡¹Dçš„å†…å®¹]\nE. [é€‰é¡¹Eçš„å†…å®¹]\nç­”æ¡ˆ: [è¿™é‡Œæ˜¯æ­£ç¡®é€‰é¡¹çš„å­—æ¯ï¼Œä¾‹å¦‚ï¼šC]\n[END]\n\n---\nç§å­é—®é¢˜å‚è€ƒï¼š\né¢˜å¹²: {seed_question['question']}\n{options_str}\nç­”æ¡ˆ: {seed_question['answer_idx']}\n---\n\nç°åœ¨ï¼Œè¯·å¼€å§‹ç”Ÿæˆï¼š"""
    
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=2048, num_return_sequences=1, do_sample=True, temperature=0.75, top_p=0.9, pad_token_id=tokenizer.eos_token_id)
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    generation_marker = "ç°åœ¨ï¼Œè¯·å¼€å§‹ç”Ÿæˆï¼š"
    model_output_only = generated_text.split(generation_marker)[-1]

    try:
        block = model_output_only.split('[START]')[1].split('[END]')[0].strip()
        parsed_data = parse_generated_text_robustly(block, gpu_id)
    except IndexError:
        tqdm.write(f"ğŸŸ¡ [GPU {gpu_id}] ID:{seed_question.get('id', 'N/A')} æœªæ‰¾åˆ°[START]/[END]æ ‡ç­¾ï¼Œå°è¯•å®½æ¾è§£æ...")
        parsed_data = parse_generated_text_robustly(model_output_only, gpu_id)

    if parsed_data:
        parsed_data.update({
            "meta_info": seed_question.get('meta_info', ''),
            "ability_path": seed_question['ability_path'],
            "generated_from_seed": seed_question['id']
        })
        return parsed_data
    else:
        tqdm.write(f"âš ï¸ [GPU {gpu_id}] ID:{seed_question.get('id', 'N/A')} æ‰€æœ‰è§£æå°è¯•å‡å¤±è´¥ã€‚")
        return None

def generation_worker(rank, world_size, gpus, config):
    gpu_id = gpus[rank]
    tqdm.write(f"--- Workerå¯åŠ¨ï¼šè¿›ç¨‹Rank {rank}ï¼Œä½¿ç”¨GPU {gpu_id} ---")

    model, tokenizer = load_local_model(config['teacher_model_path'], str(gpu_id))
    if not model: return

    full_dataset_map = load_full_dataset(config['full_dataset_path'])
    ability_to_ids_map = load_ability_classifications(config['ability_class_path'])
    ability_tree = load_ability_tree(config['ability_tree_path'])
    generation_plan = calculate_generation_allocation(ability_tree, config['num_samples'])
    
    all_tasks = []
    for ability_path, count in generation_plan.items():
        candidate_ids = ability_to_ids_map.get(ability_path)
        if candidate_ids:
            for seed_id in random.choices(candidate_ids, k=count):
                all_tasks.append({'ability_path': ability_path, 'question_data': full_dataset_map.get(seed_id)})
    
    random.seed(42 + rank) # ä¸ºæ¯ä¸ªè¿›ç¨‹è®¾ç½®ä¸åŒéšæœºç§å­
    random.shuffle(all_tasks)
    
    tasks_for_this_rank = all_tasks[rank::world_size]
    tqdm.write(f"--- [GPU {gpu_id}] åˆ†é…åˆ° {len(tasks_for_this_rank)} ä¸ªç”Ÿæˆä»»åŠ¡ ---")

    output_file = config['output_file_template'].format(rank=rank)
    with open(output_file, 'w', encoding='utf-8') as f:
        progress_bar = tqdm(tasks_for_this_rank, desc=f"ğŸš€ [GPU {gpu_id}]", position=rank, unit="æ¡")
        for task in progress_bar:
            if task['question_data']:
                task['question_data']['ability_path'] = list(task['ability_path'])
                new_question = None
                for attempt in range(config['max_retries']):
                    new_question = generate_similar_question(task['question_data'], model, tokenizer, gpu_id)
                    if new_question: break
                if new_question:
                    f.write(json.dumps(new_question, ensure_ascii=False) + '\n')

    tqdm.write(f"ğŸ‰ [GPU {gpu_id}] ä»»åŠ¡å®Œæˆï¼Œä¸´æ—¶æ–‡ä»¶å·²ä¿å­˜ã€‚")

def merge_files(file_pattern, output_file):
    input_files = sorted(glob.glob(file_pattern))
    if not input_files:
        print(f"âŒ åˆå¹¶å¤±è´¥ï¼šæœªæ‰¾åˆ°ä»»ä½•åŒ¹é… '{file_pattern}' çš„æ–‡ä»¶ã€‚")
        return
    print(f"\nğŸ” æ‰¾åˆ° {len(input_files)} ä¸ªä¸´æ—¶æ–‡ä»¶è¿›è¡Œåˆå¹¶...")

    total_lines = 0
    current_id = 0
    with open(output_file, 'w', encoding='utf-8') as outfile:
        for filename in input_files:
            with open(filename, 'r', encoding='utf-8') as infile:
                for line in infile:
                    if line.strip():
                        try:
                            data_item = json.loads(line)
                            data_item['id'] = current_id
                            outfile.write(json.dumps(data_item, ensure_ascii=False) + '\n')
                            total_lines += 1
                            current_id += 1
                        except json.JSONDecodeError:
                            print(f"âš ï¸ è­¦å‘Š: åœ¨æ–‡ä»¶ {filename} ä¸­å‘ç°æ— æ•ˆçš„JSONè¡Œï¼Œå·²è·³è¿‡ã€‚")
    print(f"âœ… åˆå¹¶å®Œæˆï¼æ€»å…±åˆå¹¶äº† {total_lines} è¡Œæ•°æ®åˆ° '{output_file}'ã€‚")

# ==============================================================================
# SECTION 2: ä¸»æ‰§è¡Œæµç¨‹
# ==============================================================================

def main():
    parser = argparse.ArgumentParser(description="æ ¹æ®å­¦ç”Ÿèƒ½åŠ›æ ‘ç”Ÿæˆé’ˆå¯¹æ€§è’¸é¦æ•°æ®ã€‚")
    parser.add_argument("--teacher_model_path", type=str, required=True, help="æ•™å¸ˆæ¨¡å‹çš„è·¯å¾„ã€‚")
    parser.add_argument("--ability_tree_path", type=str, required=True, help="å­¦ç”Ÿèƒ½åŠ›æ ‘æ–‡ä»¶è·¯å¾„ (.json)ã€‚")
    parser.add_argument("--full_dataset_path", type=str, required=True, help="ç”¨äºé€‰æ‹©ç§å­æ•°æ®çš„å®Œæ•´æ•°æ®é›†è·¯å¾„ (.jsonl)ã€‚")
    parser.add_argument("--ability_class_path", type=str, required=True, help="é—®é¢˜IDåˆ°èƒ½åŠ›è·¯å¾„çš„æ˜ å°„æ–‡ä»¶è·¯å¾„ (.jsonl)ã€‚")
    parser.add_argument("--output_path", type=str, required=True, help="ä¿å­˜ç”Ÿæˆçš„è’¸é¦æ•°æ®çš„æœ€ç»ˆè¾“å‡ºæ–‡ä»¶è·¯å¾„ (.jsonl)ã€‚")
    parser.add_argument("--num_samples", type=int, required=True, help="è¦ç”Ÿæˆçš„è’¸é¦æ•°æ®æ€»æ•°ã€‚")
    parser.add_argument("--gpu_ids", type=str, required=True, help="ç”¨äºæ¨ç†çš„GPU IDåˆ—è¡¨ï¼Œä»¥é€—å·åˆ†éš”ï¼ˆä¾‹å¦‚ '0,1,2'ï¼‰ã€‚")
    parser.add_argument("--max_retries", type=int, default=3, help="æ¯ä¸ªç”Ÿæˆä»»åŠ¡çš„æœ€å¤§é‡è¯•æ¬¡æ•°ã€‚")
    
    args = parser.parse_args()

    try:
        gpu_ids = [int(gid.strip()) for gid in args.gpu_ids.split(',')]
        if not gpu_ids: raise ValueError
        world_size = len(gpu_ids)
    except (ValueError, IndexError):
        print("é”™è¯¯: 'gpu_ids' å‚æ•°æ ¼å¼ä¸æ­£ç¡®ã€‚è¯·æä¾›ä¸€ä¸ªé€—å·åˆ†éš”çš„æ•°å­—åˆ—è¡¨ï¼Œä¾‹å¦‚ '0,1,2'ã€‚")
        return
        
    Path(args.output_path).parent.mkdir(parents=True, exist_ok=True)

    print("="*80)
    print(f"ğŸš€ STAGE 1: å¼€å§‹åˆ†å¸ƒå¼æ•°æ®ç”Ÿæˆ...")
    print(f"   å°†åœ¨ {world_size} ä¸ªGPUä¸Šå¯åŠ¨: {gpu_ids}")
    
    with tempfile.TemporaryDirectory() as temp_dir:
        config = {
            'num_samples': args.num_samples,
            'teacher_model_path': args.teacher_model_path,
            'full_dataset_path': args.full_dataset_path,
            'ability_tree_path': args.ability_tree_path,
            'ability_class_path': args.ability_class_path,
            'max_retries': args.max_retries,
            'output_file_template': os.path.join(temp_dir, "rank_{rank}.jsonl")
        }
        
        mp.spawn(generation_worker, args=(world_size, gpu_ids, config), nprocs=world_size, join=True)
        
        print("\n" + "="*80)
        print("âœ… STAGE 1: æ‰€æœ‰ç”Ÿæˆè¿›ç¨‹å‡å·²å®Œæˆã€‚")
        print("="*80)

        print(f"\nğŸš€ STAGE 2: å¼€å§‹åˆå¹¶ä¸´æ—¶æ–‡ä»¶...")
        temp_file_pattern = os.path.join(temp_dir, "rank_*.jsonl")
        merge_files(temp_file_pattern, args.output_path)

    print("\n" + "="*80)
    print("ğŸ‰ğŸ‰ğŸ‰ å…¨éƒ¨æµç¨‹æ‰§è¡Œå®Œæ¯•ï¼ ğŸ‰ğŸ‰ğŸ‰")
    print(f"æœ€ç»ˆäº§å‡ºæ–‡ä»¶ä½äº: {args.output_path}")
    print("="*80)

if __name__ == '__main__':
    main()